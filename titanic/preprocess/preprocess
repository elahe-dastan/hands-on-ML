import os
import tarfile
from six.moves import urllib
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from zlib import crc32
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from pandas.plotting import scatter_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import joblib
from sklearn.model_selection import GridSearchCV
from scipy import stats

TITANIC_PATH = os.path.join("/home/raha/py/src/hands-on-ML/titanic/datasets/")


def load_titanic_train_data(titanic_path=TITANIC_PATH):
    csv_path = os.path.join(titanic_path, "train.csv")
    return pd.read_csv(csv_path)


def load_titanic_test_data(titanic_path=TITANIC_PATH):
    csv_path = os.path.join(titanic_path, "test.csv")
    return pd.read_csv(csv_path)


def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())


data = load_titanic_train_data()
print(data.info())

test_data = load_titanic_test_data()

data_labels = data["Survived"].copy()

data = data.drop("Survived", axis=1).drop("Name", axis=1).drop("Cabin", axis=1).drop("Embarked", axis=1)
test_data = test_data.drop("Name", axis=1).drop("Cabin", axis=1).drop("Embarked", axis=1)
data_num = data.drop("Sex", axis=1).drop("Ticket", axis=1)


# a = data["Cabin"][1]
# data = data["Cabin"].fillna(a, inplace=True)


num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('std_scaler', StandardScaler()),
])

full_pipeline = ColumnTransformer([
    ("num", num_pipeline, list(data_num)),
    ("cat_sex", OrdinalEncoder(), ["Sex"]),
    ("cat_ticket", OneHotEncoder(), ["Ticket"]),
])

data_prepared = full_pipeline.fit_transform(data)
print(data_prepared)

full_pipeline = ColumnTransformer([
    ("num", num_pipeline, list(data_num)),
    ("cat_sex", OrdinalEncoder(), ["Sex"]),
    ("cat_ticket", OneHotEncoder(), ["Ticket"]),
])

test_data_prepared = full_pipeline.fit_transform(test_data)

log_reg = LogisticRegression()
log_reg.fit(data_prepared, data_labels)

# data_predictions = log_reg.predict(test_data_prepared)
# print(data_predictions)
#
# log_mse = mean_squared_error(data_labels, data_predictions)
# log_rmse = np.sqrt(log_mse)
# print(log_rmse)

# scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)
# tree_rmse_scores = np.sqrt(-scores)
# display_scores(tree_rmse_scores)
#
# lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)
# lin_rmse_scores = np.sqrt(-lin_scores)
# display_scores(lin_rmse_scores)


# joblib.dump(forest_reg, "forest_model")
# my_model = joblib.load("forest_model")

# param_grid = [
#     {'n_estimators': [3, 10, 30], 'max_features':[2, 4, 6, 8]},
# {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
# ]


# grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)
# grid_search.fit(housing_prepared, housing_labels)
# print(grid_search.best_params_)
# print(grid_search.best_estimator_)
# cvres = grid_search.cv_results_
# for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
#     print(np.sqrt(-mean_score), params)

# feature_importance = grid_search.best_estimator_.feature_importances_
# print(feature_importance)
#
# extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
# cat_encoder = full_pipeline.named_transformers_["cat"]
# cat_one_hot_attribs = list(cat_encoder.categories_[0])
# attributes = num_attribs + extra_attribs + cat_one_hot_attribs
# print(sorted(zip(feature_importance, attributes), reverse=True))

# final_model = grid_search.best_estimator_

# X_test = strat_test_set.drop("median_house_value", axis=1)
# y_test = strat_test_set["median_house_value"].copy()
#
# X_test_prepared = full_pipeline.transform(X_test)

# final_predictions = final_model.predict(X_test_prepared)
# for y, fp in zip(y_test, final_predictions):
#     print(y, fp)

# final_mse = mean_squared_error(y_test, final_predictions)
# final_rmse = np.sqrt(final_mse)
# print(final_rmse)

# confidence = 0.95
# squared_errors = (final_predictions - y_test) ** 2
# print(np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(),
#                                scale=stats.sem(squared_errors))))
